{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Availability: True\n",
      "Number of GPUs available: 1\n",
      "Number of CPU cores: 8\n",
      "GPU 0: NVIDIA GeForce RTX 3060 Ti\n",
      "Device in use: cuda\n"
     ]
    }
   ],
   "source": [
    "### Note that for this project, I referenced code from Professor Tallman an Daniel Bourke from his freeCodeCamp.com video! https://www.youtube.com/channel/UCr8O8l5cCX85Oem1d18EezQ\n",
    "\n",
    "import torch\n",
    "\n",
    "device = 'cpu'\n",
    "gpu_count = 0\n",
    "\n",
    "# Check for GPU availability\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f'GPU Availability: {gpu_available}')\n",
    "\n",
    "if gpu_available:\n",
    "    device = 'cuda'\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f'Number of GPUs available: {gpu_count}')\n",
    "    print(f'Number of CPU cores: {torch.get_num_threads()}')\n",
    "    \n",
    "    # Print each GPU's name\n",
    "    for i in range(gpu_count):\n",
    "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "\n",
    "# Print selected device\n",
    "print(f'Device in use: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (37500, 10000)\n",
      "Shape of X_test: (12500, 10000)\n",
      "Shape of y_train: (37500,)\n",
      "Shape of y_test: (12500,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "test_data = pd.read_csv(r\"data\\test_data.csv.gzip\", compression='gzip')\n",
    "train_data = pd.read_csv(r\"data\\train_data.csv.gzip\", compression='gzip')\n",
    "\n",
    "df = pd.concat([test_data, train_data], ignore_index=True)\n",
    "\n",
    "X = df.loc[:, df.columns != 'model_labels']\n",
    "y = df[\"model_labels\"]\n",
    "y = y.map({'pos': 1, 'neg': 0})\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "# Ensure y_train_tensor and y_test_tensor are integer class labels (1D)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32)  # Long type for class indices\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "# Create PyTorch datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define the PyTorch model\n",
    "class SentimentAnalysisModel(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # had to add dropout and batch normalization to stop it from overfitting!\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(input_shape, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            self.relu,\n",
    "            nn.Linear(256, 512),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.BatchNorm1d(512),\n",
    "            self.relu,\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            self.relu,\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Dropout(p=0.2)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = torch.unique(y_train_tensor).size(0)\n",
    "\n",
    "model = SentimentAnalysisModel(\n",
    "    input_shape=X_train_tensor.shape[1], \n",
    "    num_classes=num_classes\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                            lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_has_converged(loss, min_delta=0.03, stable_epoch_count=2):\n",
    "    '''\n",
    "    Determines whether the learning loss has converged to a stable value\n",
    "    '''\n",
    "    \n",
    "    # These variables persist across function calls!\n",
    "    if not hasattr(loss_has_converged, \"counter\"):\n",
    "        loss_has_converged.counter = 0\n",
    "    if not hasattr(loss_has_converged, \"best_loss\"):\n",
    "        loss_has_converged.best_loss = float('inf')\n",
    "\n",
    "    # Is the loss continuing to improve (e.g., get smaller)?\n",
    "    if loss_has_converged.best_loss - loss > min_delta:\n",
    "        loss_has_converged.best_loss = loss\n",
    "        loss_has_converged.counter = 0\n",
    "\n",
    "    # Loss has not improved; 'patience' in a row means convergence\n",
    "    else:\n",
    "        loss_has_converged.counter += 1\n",
    "        if loss_has_converged.counter >= stable_epoch_count:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    '''\n",
    "    Performs a single round of learning with the training data, updating\n",
    "    the weights and biases after it is completed according to the given\n",
    "    loss function and optimization object.\n",
    "    '''\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Add a loop to loop through the training batches\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        # Put data on gpu\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate loss and accuracy\n",
    "        loss = loss_fn(y_pred.squeeze(), y)\n",
    "        train_loss += loss\n",
    "\n",
    "        # convert logits to probabilities then to 0 or 1\n",
    "        y_pred_labels = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "        train_acc += accuracy_fn(y_true=y.cpu().detach().numpy(),\n",
    "                                y_pred=y_pred_labels.cpu().detach().numpy())\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Divide total train loss and acc by length of train dataloader\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\")\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              data_loader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "    \"\"\"Performs a testing loop step on model going over data_loader.\"\"\"\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Put the model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Turn on inference mode context manager\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Send the data to the target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass (outputs raw logits)\n",
    "            test_pred = model(X)\n",
    "\n",
    "            # 2. Calculate the loss/acc\n",
    "            test_loss += loss_fn(test_pred.squeeze(), y)\n",
    "\n",
    "            # convert logits to probabilities then to 0 or 1\n",
    "            test_pred_labels = torch.round(torch.sigmoid(test_pred))\n",
    "\n",
    "            test_acc += accuracy_fn(y_true=y.cpu().detach().numpy(),\n",
    "                                    y_pred=test_pred_labels.cpu().detach().numpy()) \n",
    "\n",
    "        # Adjust metrics and print out\n",
    "        test_loss /= len(data_loader)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "----------\n",
      "Train loss: 0.39896 | Train acc: 0.79%\n",
      "Test loss: 0.28364 | Test acc: 0.89%\n",
      "\n",
      "Epoch: 1\n",
      "----------\n",
      "Train loss: 0.29654 | Train acc: 0.84%\n",
      "Test loss: 0.27978 | Test acc: 0.89%\n",
      "\n",
      "Epoch: 2\n",
      "----------\n",
      "Train loss: 0.24995 | Train acc: 0.86%\n",
      "Test loss: 0.32697 | Test acc: 0.87%\n",
      "\n",
      "Epoch: 3\n",
      "----------\n",
      "Train loss: 0.21641 | Train acc: 0.87%\n",
      "Test loss: 0.37655 | Test acc: 0.87%\n",
      "\n",
      "Epoch: 4\n",
      "----------\n",
      "Train loss: 0.19878 | Train acc: 0.88%\n",
      "Test loss: 0.41436 | Test acc: 0.88%\n",
      "\n",
      "Epoch: 5\n",
      "----------\n",
      "Train loss: 0.18232 | Train acc: 0.88%\n",
      "Test loss: 0.42924 | Test acc: 0.87%\n",
      "\n",
      "Epoch: 6\n",
      "----------\n",
      "Train loss: 0.17728 | Train acc: 0.89%\n",
      "Test loss: 0.43894 | Test acc: 0.87%\n",
      "\n",
      "Epoch: 7\n",
      "----------\n",
      "Train loss: 0.17095 | Train acc: 0.89%\n",
      "Test loss: 0.47880 | Test acc: 0.87%\n",
      "\n",
      "Early stopping triggered at epoch 8!\n"
     ]
    }
   ],
   "source": [
    "# torch.manual_seed(42)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "# Create a optimization and evaluation loop using train_step() and test_step()\n",
    "for epoch in range(epochs):\n",
    "  print(f\"Epoch: {epoch}\\n----------\")\n",
    "\n",
    "  loss = train_step(model=model,\n",
    "             data_loader=train_loader,\n",
    "             loss_fn=loss_fn,\n",
    "             optimizer=optimizer,\n",
    "             accuracy_fn=accuracy_score,\n",
    "             device=device)\n",
    "  \n",
    "  test_step(model=model,\n",
    "            data_loader=test_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=accuracy_score,\n",
    "            device=device)\n",
    "  \n",
    "  if loss_has_converged(loss):\n",
    "        print(f\"Early stopping triggered at epoch {epoch+1}!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn,\n",
    "               device=device):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make our data device agnostic\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # Make predictions\n",
    "            y_pred = model(X)\n",
    "\n",
    "            # Accumulate the loss and acc values per batch\n",
    "            loss += loss_fn(y_pred.squeeze(), y)\n",
    "\n",
    "            # convert logits to probabilities then to 0 or 1\n",
    "            y_pred_labels = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "            acc += accuracy_fn(y_true=y.cpu().detach().numpy(),\n",
    "                                y_pred=y_pred_labels.cpu().detach().numpy())\n",
    "\n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "\n",
    "    return {\"model_name\": model.__class__.__name__,\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'SentimentAnalysisModel',\n",
       " 'model_loss': 0.47879886627197266,\n",
       " 'model_acc': 0.8711734693877551}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results = eval_model(model=model,\n",
    "                            data_loader=test_loader,\n",
    "                            loss_fn=loss_fn,\n",
    "                            accuracy_fn=accuracy_score,\n",
    "                            device=device)\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the models parameters for later use!\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
